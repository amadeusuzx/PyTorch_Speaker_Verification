{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = [\"black\",\"cancel\",\"centeralign\",\"copy\",\"large\",\"medium\",\"newslide\",\"paste\",\"red\",\"textbox\"]\n",
    "def crop(buffer, clip_len):\n",
    "    # randomly select time index for temporal jittering\n",
    "    time_index = np.random.randint(buffer.shape[1] - clip_len)\n",
    "    # randomly select start indices in order to crop the video\n",
    "\n",
    "    # crop and jitter the video using indexing. The spatial crop is performed on \n",
    "    # the entire array, so each frame is cropped in the same location. The temporal\n",
    "    # jitter takes place via the selection of consecutive frames\n",
    "    buffer = buffer[:,time_index:time_index + clip_len,:,:]\n",
    "\n",
    "    return buffer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dict()\n",
    "for c in commands:\n",
    "    dataset[c] = []\n",
    "    for f in sorted(glob(\"./100/test/\"+c+\"/*\")):\n",
    "        capture = cv2.VideoCapture(f)\n",
    "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        sample = np.linspace(0, frame_count.shape[0]-1, num=16, dtype=int)\n",
    "        if frame_count>32:\n",
    "\n",
    "            buffer = np.empty((16,80,100, 3), np.dtype('float32'))\n",
    "            count = 0\n",
    "            retaining = True\n",
    "            j=0\n",
    "            # read in each frame, one at a time into the numpy buffer array\n",
    "            while (count < frame_count and retaining):\n",
    "                retaining, frame = capture.read()\n",
    "                if count == sample[j]:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    buffer[i] = frame\n",
    "                    j+=1\n",
    "                count += 1\n",
    "            buffer = buffer.transpose((3, 0, 1, 2)) \n",
    "            buffer = (buffer - 128)/128\n",
    "            dataset[c].append(buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_path=\"/Users/soshiyuu/Devlopment/Github/amad/PyTorch_Speaker_Verification/train_tisv/\"\n",
    "ts_path=\"/Users/soshiyuu/Devlopment/Github/amad/PyTorch_Speaker_Verification/test_tisv/\"\n",
    "i=0\n",
    "for c in commands:\n",
    "    for i in range(len(dataset[c])):\n",
    "        np.save(ts_path+c+\"/\"+c+\"%d\"%i,dataset[c][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 88   3  32  80 100]\n"
    }
   ],
   "source": [
    "print(np.array(np.stack(dataset[\"black\"]).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in commands:\n",
    "    for i in range(len(dataset[c])):\n",
    "        dataset[c][i] = np.resize(np.array(dataset[c][i]),(70,80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['.DS_Store',\n 'speaker8.npy',\n 'speaker9.npy',\n 'speaker10.npy',\n 'speaker4.npy',\n 'speaker5.npy',\n 'speaker7.npy',\n 'speaker6.npy',\n 'speaker2.npy',\n 'speaker3.npy',\n 'speaker1.npy']"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "os.listdir(tr_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['paste33.npy', 'paste24.npy', 'paste73.npy', 'paste13.npy', 'paste48.npy']\n"
    }
   ],
   "source": [
    "import random\n",
    "utter_num = 5\n",
    "path = \"/Users/soshiyuu/Devlopment/Github/amad/PyTorch_Speaker_Verification/train_tisv/\"\n",
    "np_file_dir = [b for b in os.listdir(path) if b[0] != \".\"]\n",
    "selected_dir = random.sample(np_file_dir, 1)[0]  # select random speaker          \n",
    "np_file_list = os.listdir(os.path.join(path+selected_dir))\n",
    "selected_file = random.sample(np_file_list,utter_num)\n",
    "\n",
    "print(selected_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utters = np.load(os.path.join(path, selected_file))        # load utterance spectrogram of selected speaker\n",
    "utter_index = np.random.randint(0, utters.shape[0], self.utter_num)   # select M utterances per speaker\n",
    "utterance = utters[utter_index]     "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit8fc386767f5342a8a561d4fed70b47ff",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}